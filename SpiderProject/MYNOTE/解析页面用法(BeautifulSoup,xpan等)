BeautifulSoup解析页面用法

页面编码问题处理方式:
目的:用于后面页面解析时,读出的text值是汉字,其他编码,都要进行处理;
编码问题处理函数:
def decode_html(html, charsets=('gbk', 'utf-8')):
    # 编码处理
    page_html = ''
    for charset in charsets:
        try:
            page_html = html.decode(charset)
            break
        except Exception as e:
            pass
    return page_html
在进行页面爬取是你可以到页面去查看其编码方式,知道编码方式,直接

1,获取页面
  - 头部:
    header = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko)Chrome/65.0.3325.181 Safari/537.36'}
  - 拿取页面;
  方式一:
    req = Request(url=url, headers=header)
    r = urlopen(req)
    这里的r.read()就是这个页面,函数处理就是右边的代码
    res = decode_html(r.read()) / res=res.read().decode('gbk') 
    soup = BeautifulSoup(res)


  方式二:
  requests.get:默认是utf-8编码方式,这里页面是gbk,所以需要做处理;
    res = requests.get(url, headers=header)  
    res.encoding='gbk'  处理编码方式:这里是res.text是页面
    soup = BeautifulSoup(res.text, 'lxml')


    这样解析页面不会乱码

解析方式:
	1,找到想要获取页面区域数据最大的一个div(find),定位class/id;
	msg = soup.find('div',{'class':'f14list'})

	2,拿到以后获取,子节点(下面所有的li):
	li = msg.findAll('li')

	3,拿去li下面的数据,遍历循环处理;
	拿其中的a标签
	a = li[0].find('a')
	a.text   
	a.get('href') 

	4,拿去li下面的span标签(多个);
	spans = li[0].findAll('span')  
	spans[0].text  
	spans[0].get('href')

	5,拿去li下面指定的标签
	li[0].find('span',{'class':'star'})

这样的方式解析数据,无论页面中那个div里面有多少数据,在哪里我们可以找到并获取到,并可以拿去其属性和text,实在不知道怎么获取,可以采用debug方式,在控制台进行打印,里面有提示.



xpan页面解析方式:

1,获取页面(res);

tem = etree.HTML(res)

2,解析;
xpath获取的对象不是一个页面形式,它是一个对象形式,在获取值得时候使用:
li.text/li.href/li.class等
while True:
    如果想取到所有的li标签,这里要对li[1]进行处理
	# li = tem.xpath('//*[@id="article-container"]/div[2]/div[1]/li[1]/h1/text()')
	li = tem.xpath('//*[@id="article-container"]/div[2]/div[1]/li/h1')
	这里拿到的h1对象是一个列表,这里有个规律:想获取那个循环标签下面的对象/值/属性等,将循环体不做准确定位(下标取值,也就是单独的li),这样处理就可以获取循环体下面的某一部分数据;



3,selenium模式获取数据
自动测试,获取,点击等
from selenium import webdriver

# 设置驱动位置
chromdriver = 'C:\Program Files (x86)\Google\Chrome\Application\chromedriver.exe'

# 模拟哪个浏览器行为
browser = webdriver.Chrome(chromdriver)

# 打开url地址
browser.get('https://www.taobao.com/')

页面数据解析(常用):
	获取class=service-bd的标签下面的li下面的a
	lis = browser.find_elements_by_css_selector('.service-bd li a')
	拿class
	browser.find_element_by_class_name()
	拿id
	browser.find_element_by_id()
	拿xpath
	browser.find_element_by_xpath()
这里的element可换成elements,这里获取的是一个对象,对象集合

	获取具体数据
	属性
	i.get_attribute('href')
	值
	i.text
	可继续往下获取对象
	i.find_element_by_id()
	点击事件
	i.click()

4,scrapy框架数据获取
.............

 cut = res.xpath('//*[@id="content"]/div/div[1]/ol/li/div/div[2]/div[2]/p[1]/text()[1]').extract()

















